# Self-Harm Content Detection using NLP

This project focuses on detecting **self-harm related textual content** using Natural Language Processing (NLP) techniques. Multiple models were implemented and compared, including a traditional **Logistic Regression** classifier and a **BERT-based deep learning model**, with BERT achieving superior performance.

The goal is to explore how modern transformer-based models outperform classical approaches in sensitive text classification tasks.

---

## ðŸ“Œ Project Overview

Online platforms often struggle to identify self-harm content early. This project aims to:

- Preprocess and clean raw text data
- Perform exploratory data analysis (EDA)
- Train and evaluate multiple NLP models
- Compare traditional ML models with transformer-based models
- Analyze performance differences and results

---

## ðŸ§  Models Used

### 1. Logistic Regression
- TF-IDF based feature extraction
- Baseline classical NLP model
- Faster training, lower computational cost

### 2. BERT (Bidirectional Encoder Representations from Transformers)
- Context-aware embeddings
- Fine-tuned for classification
- Significantly better performance on nuanced text

---
